{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baa9ff6-9640-4998-84cc-71960c627272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Environment Configuration (Model C: Pure RL)\n",
    "# Imports necessary libraries for Deep Reinforcement Learning (DRL), numerical computation, and visualization.\n",
    "\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Check for CUDA availability to enable GPU acceleration\n",
    "torch_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Torch CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af76e19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Module Import from Definition Notebook\n",
    "# Enables importing functions and classes directly from Jupyter Notebook files (.ipynb).\n",
    "\n",
    "import import_ipynb\n",
    "\n",
    "# Imports the Quantum Environment ('qc') and the Optimization Function ('opt_classifier')\n",
    "# from the definition notebook 'model_c_definition_rl.ipynb'.\n",
    "from model_c_definition_rl  import qc, opt_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788a588f-64b7-4f21-980c-4567263279fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Experience Replay Memory\n",
    "# Implements a cyclic buffer to store transitions (experiences) for off-policy training.\n",
    "# This breaks temporal correlations in the data sequence, stabilizing the learning process.\n",
    "\n",
    "# Define a named tuple to represent a single transition in the MDP: (state, action, next_state, reward, done)\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \"\"\"\n",
    "    A cyclic buffer that stores experiences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"\n",
    "        Initializes the replay memory with a fixed capacity.\n",
    "        Args:\n",
    "            capacity (int): Maximum number of transitions to store.\n",
    "        \"\"\"\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"\n",
    "        Saves a transition to memory.\n",
    "        \"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Randomly samples a batch of transitions from memory.\n",
    "        Required for Stochastic Gradient Descent (SGD) updates.\n",
    "        \"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the current number of elements in memory.\n",
    "        \"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f37681-ec8d-4afd-8791-49049f2ae1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Deep Q-Network (DQN) Model Definition\n",
    "# Defines the neural network architecture used to approximate the Q-value function.\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    A Multi-Layer Perceptron (MLP) serving as the function approximator for Q-learning.\n",
    "    Maps state observations to Q-values for each possible action.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        \"\"\"\n",
    "        Initializes the network layers.\n",
    "        Args:\n",
    "            n_observations: Dimension of the input state space (flattened observation vector).\n",
    "            n_actions: Dimension of the action space (number of candidate gates).\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.flatten = nn.Flatten() # Flattens multidimensional input tensors\n",
    "        self.layer1 = nn.Linear(n_observations, 256) # Hidden Layer 1: Input -> 256 units\n",
    "        self.layer2 = nn.Linear(256, 256)            # Hidden Layer 2: 256 -> 256 units\n",
    "        self.layer3 = nn.Linear(256, n_actions)      # Output Layer: 256 -> Action Q-values\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        Applies ReLU activation to hidden layers.\n",
    "        \"\"\"\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.layer1(x)) # Activation function: Rectified Linear Unit (ReLU)\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)      # Returns raw Q-values (no activation at output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3ceb8a-3b2b-4d42-8380-570fb372a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Hyperparameters Configuration\n",
    "# Defines key parameters controlling the Reinforcement Learning process.\n",
    "\n",
    "BATCH_SIZE = 128     # Number of transitions sampled from replay memory per update step\n",
    "GAMMA = 0.8          # Discount factor for future rewards (0.8 emphasizes near-term rewards)\n",
    "EPS_START = 0.9      # Initial Epsilon value for epsilon-greedy policy (High exploration)\n",
    "EPS_END = 0.05       # Minimum Epsilon value (Low exploration, high exploitation)\n",
    "EPS_DECAY = 500      # Rate of Epsilon decay (Slower decay due to large action space: 200 -> 500)\n",
    "TAU = 0.005          # Soft update rate for target network parameters\n",
    "LR = 0.001           # Learning Rate for the Adam optimizer\n",
    "num_episodes = 500   # Total number of training episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf6d79a-b3f8-49e7-b1f7-84db3565964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Environment Initialization\n",
    "# Sets up the quantum circuit environment and defines state/action dimensions.\n",
    "\n",
    "# Initialize the RL environment (Quantum Circuit Search)\n",
    "env = qc()\n",
    "\n",
    "# Define Action Space Size (Number of candidate gates)\n",
    "n_actions = env.act_space \n",
    "\n",
    "# Note: Unlike Model B, Model C (Pure RL) does NOT use 'weights_ent_global'.\n",
    "# No pre-trained entanglement parameters are initialized or passed.\n",
    "\n",
    "# Reset environment to starting state (No arguments required for Model C)\n",
    "env.reset()\n",
    "\n",
    "# Define Observation Space Size\n",
    "# Flattens the observation matrix (Circuit Depth x Gate Features) into a 1D vector.\n",
    "# env.obs is a list of [Rot, CNOT, Param1, Param2], so length * 4 gives the total input dimension for DQN.\n",
    "n_observations = len(env.obs * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e87d09-1648-49ee-83dc-958380707906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Network Initialization and Optimizer Setup\n",
    "# Initializes the Policy Network and Target Network for the DQN algorithm.\n",
    "# The Target Network provides stable Q-value targets (Fixed Q-Targets) to prevent oscillation.\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict()) # Synchronize initial weights\n",
    "\n",
    "# Optimizer: AdamW (Adam with Weight Decay Fix) is used for parameter updates.\n",
    "# 'amsgrad=True' ensures long-term convergence properties.\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "# Initialize Experience Replay Buffer with a capacity of 10,000 transitions.\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0 # Global step counter for Epsilon decay scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a69735-8e55-4e7d-b680-bca1e73a99e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Action Selection Strategy (Epsilon-Greedy)\n",
    "# Implements the policy for selecting actions based on the current state.\n",
    "# Balances Exploration (random action) and Exploitation (best known action) using an epsilon decay schedule.\n",
    "\n",
    "def select_action(state):\n",
    "    \"\"\"\n",
    "    Selects an action according to the Epsilon-Greedy policy.\n",
    "    Args:\n",
    "        state (tensor): The current state observation.\n",
    "    Returns:\n",
    "        tensor: The index of the selected action.\n",
    "    \"\"\"\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    \n",
    "    # Calculate Epsilon Threshold (Exponential Decay)\n",
    "    # Threshold decreases as training progresses, shifting focus from exploration to exploitation.\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    \n",
    "    # Exploitation: Select the action with the highest Q-value from the Policy Network\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) returns (values, indices). We select [1] for indices (actions).\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    \n",
    "    # Exploration: Select a random action from the action space\n",
    "    else:\n",
    "        return torch.tensor([[env.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200154ae-b0fc-4e82-a785-0146955ffecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Model Optimization Function\n",
    "# Performs a single step of Stochastic Gradient Descent (SGD) to update the Policy Network.\n",
    "# Utilizes Experience Replay and Fixed Q-Targets to stabilize training.\n",
    "\n",
    "def optimize_model():\n",
    "    \"\"\"\n",
    "    Optimizes the policy network by minimizing the loss between predicted Q-values and target Q-values.\n",
    "    Implements the Bellman Optimality Equation: Q(s,a) = r + gamma * max(Q(s',a'))\n",
    "    \"\"\"\n",
    "    # 1. Check Replay Memory Size\n",
    "    # Ensure there are enough samples to form a batch.\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    # 2. Sample a Batch of Transitions\n",
    "    # Randomly sample 'BATCH_SIZE' transitions to break temporal correlations.\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    # Converts batch-array of Transitions to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # 3. Prepare Tensor Batches\n",
    "    # Concatenate state, action, and reward tensors for batch processing.\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # 4. Compute Current Q-Values (Q(s_t, a_t))\n",
    "    # Forward pass through Policy Network and select Q-values corresponding to taken actions.\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    # 5. Compute Next State Values (V(s_{t+1}) = max_a Q(s_{t+1}, a))\n",
    "    # Initialize next state values to zero.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    \n",
    "    # Use Target Network to compute Q-values for next states (Fixed Q-Targets).\n",
    "    # We detach from the graph because target values are treated as constants (no gradient).\n",
    "    with torch.no_grad():\n",
    "        next_state_values = target_net(torch.cat(batch.next_state)).max(1)[0]\n",
    "\n",
    "    # 6. Compute Expected Q-Values (Bellman Target)\n",
    "    # Formula: y = r + gamma * max(Q_target(s', a'))\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # 7. Compute Loss\n",
    "    # Use Huber Loss (SmoothL1Loss) which is less sensitive to outliers than MSE.\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # 8. Backpropagation and Optimization\n",
    "    optimizer.zero_grad() # Clear previous gradients\n",
    "    loss.backward()       # Compute gradients\n",
    "    \n",
    "    # Gradient Clipping: Clamps gradients to [-100, 100] to prevent exploding gradients.\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    \n",
    "    optimizer.step()      # Update network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1774e06e-bdd8-4086-917a-a9361289147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Training Log Initialization\n",
    "# Initializes lists to store performance metrics and circuit data across all episodes.\n",
    "\n",
    "# Reward tracking\n",
    "reward_ep_list = []      # Detailed reward components per step\n",
    "reward_sum_ep_list = []  # Total cumulative reward per episode\n",
    "\n",
    "# Circuit and State tracking\n",
    "obs_ep_list = []         # History of observation vectors (circuit structures)\n",
    "outs_ep_list = []        # Optimization logs (cost/accuracy per iteration)\n",
    "figset_ep_list = []      # Data snapshots for visualization\n",
    "final_ep_list = []       # Final state summaries\n",
    "\n",
    "# Performance Metrics tracking (Accuracy & Cost)\n",
    "acc_train_last_ep = []\n",
    "acc_val_last_ep   = []\n",
    "acc_train_max_ep  = []\n",
    "acc_val_max_ep    = []\n",
    "cost_ep = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baefd799-d716-4d1a-b2f8-588ab128182c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 11: Main Training Loop\n",
    "# Executes the reinforcement learning process over a specified number of episodes.\n",
    "# Interactions between the agent (DQN) and the environment (Quantum Circuit) occur here.\n",
    "\n",
    "# Measure execution time for performance profiling\n",
    "# %%time\n",
    "\n",
    "tau = 0.05 # Soft update parameter (redundant if TAU is defined in Cell 5, but kept for consistency)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # 1. Episode Initialization\n",
    "    # Reset the environment to start a new circuit search.\n",
    "    # Note: Unlike Model B, Model C does not pass 'weights_ent_global' to reset().\n",
    "    env.reset()\n",
    "    \n",
    "    # Get initial state and wrap it as a tensor\n",
    "    state = env.obs\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    # Initialize episode-specific logging lists\n",
    "    reward_list = []; obs_list = []; outs_list = []; figset_list = []\n",
    "    \n",
    "    # 2. Step-by-Step Interaction\n",
    "    for t in count():\n",
    "        # Action Selection (Epsilon-Greedy)\n",
    "        action = select_action(state)\n",
    "        \n",
    "        # Execute Action in the Environment\n",
    "        # env.step() updates the circuit and returns success status (True/False).\n",
    "        truncated = not env.step(action.item())\n",
    "\n",
    "        if truncated:\n",
    "            print('truncated error')\n",
    "            break\n",
    "        \n",
    "        # Retrieve Transition Data\n",
    "        observation = env.obs\n",
    "        reward = env.reward\n",
    "        terminated = env.term\n",
    "        done = env.done\n",
    "\n",
    "        # Tensor conversion for storage in Replay Memory\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        # Store Transition in Replay Memory\n",
    "        memory.push(state, action, next_state, reward, done)\n",
    "        \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # 3. Model Optimization (DQN Update)\n",
    "        optimize_model()\n",
    "\n",
    "        # 4. Target Network Soft Update\n",
    "        # Slowly update target network parameters towards policy network parameters.\n",
    "        # theta_target = tau * theta_policy + (1 - tau) * theta_target\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        \n",
    "        # 5. Logging (Per Step)\n",
    "        reward_list.append(float(reward))\n",
    "        obs_list.append(env.draw)     # Store circuit diagram\n",
    "        outs_list.append(env.outs)    # Store optimization logs\n",
    "        figset_list.append(env.figset)# Store model weights/data\n",
    "        \n",
    "        # 6. Episode Termination Handling\n",
    "        if terminated:\n",
    "            # Extract performance metrics from the final optimization log\n",
    "            cost_series = [row[1] for row in env.outs]\n",
    "            acc_train_series = [row[2] for row in env.outs]\n",
    "            acc_val_series   = [row[3] for row in env.outs]\n",
    "\n",
    "            acc_train_last = float(acc_train_series[-1])\n",
    "            acc_val_last   = float(acc_val_series[-1])\n",
    "            acc_train_max  = float(max(acc_train_series))\n",
    "            acc_val_max    = float(max(acc_val_series))\n",
    "            cost_val = float(cost_series[-1])\n",
    "\n",
    "            # Append episode metrics to global lists\n",
    "            acc_train_last_ep.append(acc_train_last)\n",
    "            acc_val_last_ep.append(acc_val_last)\n",
    "            acc_train_max_ep.append(acc_train_max)\n",
    "            acc_val_max_ep.append(acc_val_max)\n",
    "            cost_ep.append(cost_val)\n",
    "\n",
    "            # Identify the best step within the episode (highest reward)\n",
    "            max_value = max(reward_list)\n",
    "            max_index = reward_list.index(max_value)\n",
    "            \n",
    "            # Store metrics corresponding to the best step\n",
    "            reward_ep_list.append(max_value)\n",
    "            reward_sum_ep_list.append(sum(reward_list))\n",
    "            obs_ep_list.append(obs_list[max_index])\n",
    "            outs_ep_list.append(outs_list[max_index])\n",
    "            \n",
    "            # Store visualization data for the best step and the final step\n",
    "            # Note: Model C stores 'env.gatestream' directly as it contains the full circuit structure.\n",
    "            figset_ep_list.append([figset_list[max_index], env.gatestream[:max_index+1]])\n",
    "            final_ep_list.append([figset_list[-1], env.gatestream[:t+1]])\n",
    "\n",
    "            # 7. Final Episode Processing (Visualization & Saving)\n",
    "            if i_episode == num_episodes - 1:\n",
    "                # Re-run optimization/drawing for the final circuit of the last episode\n",
    "                # Note: opt_classifier call uses only gatestream (no weights_ent).\n",
    "                outs_final, draw_p, _ = opt_classifier(\n",
    "                    env.gatestream,\n",
    "                    iters=0, draw=True\n",
    "                )\n",
    "                if draw_p is not None:\n",
    "                    print(f\"\\n=== Final circuit (episode #{i_episode+1}, steps={t+1}) ===\")\n",
    "                    print(draw_p)\n",
    "                    with open(\"final_circuit.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(draw_p)\n",
    "\n",
    "                # Save the final gate sequence\n",
    "                with open(\"final_gatestream.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(env.gatestream, f, ensure_ascii=False, indent=2)\n",
    "                    \n",
    "            print(f\"Episode complete : {i_episode+1} ({t+1}) | cost={cost_val:.3f}, acc_val_last={acc_val_last:.3f}, acc_val_max={acc_val_max:.3f}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b80574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Training Performance Visualization (Cumulative Reward)\n",
    "# Plots the total reward accumulated per episode to analyze the agent's learning convergence.\n",
    "# An upward trend indicates that the agent is successfully learning to construct better circuits.\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward sum')\n",
    "plt.plot(reward_sum_ep_list) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8469ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Training Performance Visualization (Maximum Reward)\n",
    "# Plots the maximum reward achieved in each episode.\n",
    "# This metric helps identify if the agent is discovering high-quality solutions (peak performance),\n",
    "# even if the average performance fluctuates due to exploration.\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Max Reward')\n",
    "plt.plot(reward_ep_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b6651a-0634-45d0-8a14-0dc6d3a8efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Data Persistence (Saving Experiment Results)\n",
    "# Serializes and saves all training metrics, circuit structures, and performance logs to disk.\n",
    "# This ensures that the experimental results are preserved for post-training analysis and visualization.\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save Reward Metrics (Training Trajectory)\n",
    "with open('reward_ep_list_non.pkl', 'wb') as f: pickle.dump(reward_ep_list, f)\n",
    "with open('reward_sum_ep_list_non.pkl', 'wb') as f: pickle.dump(reward_sum_ep_list, f)\n",
    "\n",
    "# Save Circuit Architectures and Optimization Logs\n",
    "with open('obs_ep_list_non.pkl', 'wb') as f: pickle.dump(obs_ep_list, f)\n",
    "with open('outs_ep_list_non.pkl', 'wb') as f: pickle.dump(outs_ep_list, f)\n",
    "\n",
    "# Save Visualization Data and Final Model States\n",
    "with open('figset_ep_list_non.pkl', 'wb') as f: pickle.dump(figset_ep_list, f)\n",
    "with open('final_ep_list_non.pkl', 'wb') as f: pickle.dump(final_ep_list, f)\n",
    "\n",
    "# Save Detailed Performance Metrics (Accuracy and Cost)\n",
    "with open('acc_val_max_ep_non.pkl', 'wb') as f: pickle.dump(acc_val_max_ep, f)\n",
    "with open('acc_val_last_ep_non.pkl', 'wb') as f: pickle.dump(acc_val_last_ep, f)\n",
    "with open('cost_ep_non.pkl', 'wb') as f: pickle.dump(cost_ep, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
