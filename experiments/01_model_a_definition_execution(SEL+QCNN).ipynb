{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c37757-29af-4a3b-b457-e62d1bc08c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports & basic config\n",
    "import os, datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from pennylane.templates.layers import StronglyEntanglingLayers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pennylane as qml\n",
    "\n",
    "# Hyperparameters\n",
    "N_QUBITS   = 6\n",
    "FEAT_DIM   = 64     # 2**6\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS     = 500\n",
    "LR         = 1e-3\n",
    "PRINT_EVERY = 1\n",
    "\n",
    "DEVICE = torch.device(\"cpu\") \n",
    "LOG_FILE = \"output.txt\"\n",
    "\n",
    "assert FEAT_DIM == 2**N_QUBITS, \"Input dimension must be 64 (=2**6).\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298ad448-9308-4ab3-998e-042c4b9da2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load dataset\n",
    "# Ensure the 'dataset' folder contains 'data.npy' and 'labels.npy'.\n",
    "\n",
    "def load_data(folder_path):\n",
    "    data = np.load(os.path.join(folder_path, 'data_speck.npy'))\n",
    "    labels = np.load(os.path.join(folder_path, 'labels_speck.npy'))\n",
    "    return data, labels\n",
    "\n",
    "# Load data from the specified folder\n",
    "datas, labels = load_data('dataset')\n",
    "\n",
    "# Split dataset into training and validation sets (80/20 split)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "    datas, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Validation data shape: {val_data.shape}\")\n",
    "\n",
    "# Prepare Torch Tensors & DataLoaders\n",
    "Xtr = torch.from_numpy(train_data.astype(np.float32))\n",
    "ytr = torch.from_numpy(train_labels.astype(np.int64))\n",
    "Xva = torch.from_numpy(val_data.astype(np.float32))\n",
    "yva = torch.from_numpy(val_labels.astype(np.int64))\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(Xtr, ytr), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(Xva, yva), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "data_size = len(Xtr) + len(Xva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b867b4-e606-40cf-9de9-1f98fe2f9c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Quantum Circuit Definitions\n",
    "# Defines the Convolutional and Pooling layers for the QCNN (Model A).\n",
    "\n",
    "# Initialize PennyLane device\n",
    "# Using 'lightning.qubit' for faster CPU simulation.\n",
    "try:\n",
    "    dev = qml.device(\"lightning.qubit\", wires=N_QUBITS)\n",
    "except Exception:\n",
    "    dev = qml.device(\"default.qubit\", wires=N_QUBITS)\n",
    "\n",
    "def circuit8_block(thetas, wires):\n",
    "    \"\"\"\n",
    "    Convolutional Block (Unitary).\n",
    "    Applies parameterized rotations and entanglements.\n",
    "    \"\"\"\n",
    "    a, b = wires\n",
    "    # left locals\n",
    "    qml.RZ(thetas[0], wires=a); qml.RZ(thetas[1], wires=b)\n",
    "    qml.RY(thetas[2], wires=a); qml.RY(thetas[3], wires=b)\n",
    "    qml.RZ(thetas[4], wires=a); qml.RZ(thetas[5], wires=b)\n",
    "\n",
    "    qml.CNOT(wires=[a, b])\n",
    "\n",
    "    # middle locals\n",
    "    qml.RY(thetas[6], wires=a); qml.RZ(thetas[7], wires=b)\n",
    "\n",
    "    qml.CNOT(wires=[b, a])\n",
    "\n",
    "    qml.RY(thetas[8], wires=a)\n",
    "\n",
    "    qml.CNOT(wires=[a, b])\n",
    "\n",
    "    # right locals\n",
    "    qml.RZ(thetas[9],  wires=a); qml.RZ(thetas[10], wires=b)\n",
    "    qml.RY(thetas[11], wires=a); qml.RY(thetas[12], wires=b)\n",
    "    qml.RZ(thetas[13], wires=a); qml.RZ(thetas[14], wires=b)\n",
    "\n",
    "def pooling_block(phi2, wires, keep=\"left\"):\n",
    "    \"\"\"\n",
    "    Pooling Layer.\n",
    "    Measures one qubit and controls the other to reduce dimensionality.\n",
    "    \"\"\"\n",
    "    a, b = wires\n",
    "    if keep == \"left\":\n",
    "        drop, keepq = b, a\n",
    "    else:\n",
    "        drop, keepq = a, b\n",
    "    qml.CRZ(phi2[0], wires=[drop, keepq])\n",
    "    qml.CRX(phi2[1], wires=[drop, keepq])\n",
    "\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\", diff_method=\"best\")\n",
    "def qc_scouter_qnode(x,\n",
    "                     weights,\n",
    "                     theta_c1, \n",
    "                     theta_c2, \n",
    "                     theta_24,  \n",
    "                     theta_02,  \n",
    "                     phi_p1,     \n",
    "                     phi_p2,     \n",
    "                     phi_p3):    \n",
    "\n",
    "    # Amplitude Embedding for input data\n",
    "    qml.AmplitudeEmbedding(x, wires=range(6), normalize=True)\n",
    "    \n",
    "    # Strongly Entangling Layers (Initial Feature Map)\n",
    "    qml.StronglyEntanglingLayers(weights, wires=range(6))\n",
    "    \n",
    "    # Convolutional Layer 1\n",
    "    for a, b in [(0,1), (2,3), (4,5)]:\n",
    "        circuit8_block(theta_c1, wires=[a,b])     \n",
    "    \n",
    "    # Convolutional Layer 2\n",
    "    for a, b in [(1,2), (3,4), (5,0)]:\n",
    "        circuit8_block(theta_c2, wires=[a,b])        \n",
    "\n",
    "    # Pooling Layer 1\n",
    "    pooling_block(phi_p1, wires=[0,1], keep=\"left\") \n",
    "    pooling_block(phi_p1, wires=[2,3], keep=\"left\")  \n",
    "    pooling_block(phi_p1, wires=[4,5], keep=\"left\") \n",
    "\n",
    "    # Deep Convolution & Pooling (Layer 3 & 4)\n",
    "    circuit8_block(theta_24, wires=[2,4])\n",
    "    pooling_block(phi_p2, wires=[2,4], keep=\"left\")  \n",
    "\n",
    "    circuit8_block(theta_02, wires=[0,2])\n",
    "    pooling_block(phi_p3, wires=[0,2], keep=\"left\")  \n",
    "\n",
    "    # Measurement\n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1fd4c4-e3b2-49c0-b319-a2b1f9decf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: QCScouter Model Definition\n",
    "\n",
    "class QCScouter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize parameters (Weights & Rotations)\n",
    "        self.weights = nn.Parameter(0.01 * torch.randn((2,6,3)))\n",
    "        self.theta_c1 = nn.Parameter(0.01 * torch.randn(15))\n",
    "        self.theta_c2 = nn.Parameter(0.01 * torch.randn(15))\n",
    "        self.theta_24 = nn.Parameter(0.01 * torch.randn(15))\n",
    "        self.theta_02 = nn.Parameter(0.01 * torch.randn(15))\n",
    "        self.phi_p1   = nn.Parameter(0.01 * torch.randn(2))\n",
    "        self.phi_p2   = nn.Parameter(0.01 * torch.randn(2))\n",
    "        self.phi_p3   = nn.Parameter(0.01 * torch.randn(2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Handle 1D input case\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "            \n",
    "        z_list = []\n",
    "        # Iterate over batch (Manual QNode execution per sample)\n",
    "        for xi in x:\n",
    "            z = qc_scouter_qnode(\n",
    "                xi, self.weights,\n",
    "                self.theta_c1, self.theta_c2,\n",
    "                self.theta_24, self.theta_02,\n",
    "                self.phi_p1,   self.phi_p2,   self.phi_p3\n",
    "            )\n",
    "            z_list.append(z)\n",
    "            \n",
    "        # Stack results and shape into logits for CrossEntropy\n",
    "        z = torch.stack(z_list).unsqueeze(1)     \n",
    "        logits = torch.cat([z, -z], dim=1)  # [z, -z] for 2-class classification      \n",
    "        return logits\n",
    "\n",
    "model = QCScouter() \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312237dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: validate (simple)\n",
    "@torch.no_grad()\n",
    "def validate(model, loss_fn, val_loader):\n",
    "    model.eval() # Switch to evaluation mode\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_sum = 0.0\n",
    "    \n",
    "    for xb, yb in val_loader:\n",
    "        xb = xb.to(DEVICE); yb = yb.to(DEVICE) # Move batch to device\n",
    "        \n",
    "        logits = model(xb) # Forward pass\n",
    "        loss = loss_fn(logits, yb)\n",
    "        loss_sum += loss.item() * xb.size(0) # Accumulate batch loss\n",
    "        \n",
    "        pred = torch.argmax(logits, dim=1) # Get predicted class indices\n",
    "        correct += (pred == yb).sum().item() # Count correct predictions\n",
    "        total += xb.size(0)\n",
    "        \n",
    "    # Return average loss and accuracy (with division safety)\n",
    "    return (loss_sum / max(1,total)), (correct / max(1,total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e97dbc-7d61-44a8-b627-70ee94eda27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Training Loop & Logging (No Scheduler)\n",
    "\n",
    "chart = []\n",
    "\n",
    "# Log start time and dataset size\n",
    "with open(LOG_FILE, \"a\") as f:\n",
    "    f.write('\\no ' + str(datetime.datetime.now()) + f' {data_size} samples (6q)\\n')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train() # Set model to training mode\n",
    "    last_train_loss = None\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(DEVICE); yb = yb.to(DEVICE) # Move batch to device\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True) # Reset gradients\n",
    "        \n",
    "        logits = model(xb) # Forward pass\n",
    "        loss = loss_fn(logits, yb) # Compute loss\n",
    "        loss.backward() # Backpropagation\n",
    "        optimizer.step() # Update parameters\n",
    "        \n",
    "        last_train_loss = loss.item()\n",
    "\n",
    "    # Perform validation\n",
    "    val_loss, val_acc = validate(model, loss_fn, val_loader)\n",
    "\n",
    "    # Format output string\n",
    "    output = f\"Epoch {epoch+1:3d} : Train Loss= {last_train_loss:.4f}, Validation Loss= {val_loss:.4f}, Accuracy= {val_acc:.4f}\"\n",
    "    chart.append(output)\n",
    "    \n",
    "    # Append to log file\n",
    "    with open(LOG_FILE, \"a\") as f:\n",
    "        f.write(output + '\\n')\n",
    "        \n",
    "    # Print progress\n",
    "    if (epoch+1) % PRINT_EVERY == 0:\n",
    "        print(output)\n",
    "\n",
    "# Final Logging: End time, Optimizer state, and Parameter counts\n",
    "with open(LOG_FILE, \"a\") as f:\n",
    "    f.write('~ ' + str(datetime.datetime.now()) + '\\n' + str(optimizer) + '\\n')\n",
    "    f.write(\"Param Total \" + str(sum(p.numel() for p in model.parameters())) + '\\n')\n",
    "    for name, p in model.named_parameters():\n",
    "        f.write('    ' + str(name) + ' ' + str(p.numel()) + '\\n')\n",
    "\n",
    "print(\"Done. Logs ->\", LOG_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
