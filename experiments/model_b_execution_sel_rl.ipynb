{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baa9ff6-9640-4998-84cc-71960c627272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Environment Configuration\n",
    "# Imports necessary libraries for Deep Reinforcement Learning (DRL), numerical computation, and visualization.\n",
    "\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Check for CUDA availability to enable GPU acceleration\n",
    "torch_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Torch CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af76e19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Module Import from Definition Notebook\n",
    "# Enables importing functions and classes directly from Jupyter Notebook files (.ipynb).\n",
    "\n",
    "import import_ipynb\n",
    "\n",
    "# Imports the Quantum Environment ('qc') and the Optimization Function ('opt_classifier')\n",
    "# from the definition notebook 'model_b_definition_sel_rl.ipynb'.\n",
    "from model_b_definition_sel_rl import qc, opt_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788a588f-64b7-4f21-980c-4567263279fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Experience Replay Memory\n",
    "# Implements a cyclic buffer to store transitions (experiences) for off-policy training.\n",
    "# This breaks temporal correlations in the data sequence, stabilizing the learning process.\n",
    "\n",
    "# Define a named tuple to represent a single transition in the MDP: (state, action, next_state, reward, done)\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \"\"\"\n",
    "    A cyclic buffer that stores experiences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"\n",
    "        Initializes the replay memory with a fixed capacity.\n",
    "        Args:\n",
    "            capacity (int): Maximum number of transitions to store.\n",
    "        \"\"\"\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"\n",
    "        Saves a transition to memory.\n",
    "        \"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Randomly samples a batch of transitions from memory.\n",
    "        Required for Stochastic Gradient Descent (SGD) updates.\n",
    "        \"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the current number of elements in memory.\n",
    "        \"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f37681-ec8d-4afd-8791-49049f2ae1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Deep Q-Network (DQN) Model Definition\n",
    "# Defines the neural network architecture used to approximate the Q-value function.\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    A Multi-Layer Perceptron (MLP) serving as the function approximator for Q-learning.\n",
    "    Maps state observations to Q-values for each possible action.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        \"\"\"\n",
    "        Initializes the network layers.\n",
    "        Args:\n",
    "            n_observations: Dimension of the input state space (flattened observation vector).\n",
    "            n_actions: Dimension of the action space (number of candidate gates).\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.flatten = nn.Flatten() # Flattens multidimensional input tensors\n",
    "        self.layer1 = nn.Linear(n_observations, 256) # Hidden Layer 1: Input -> 256 units\n",
    "        self.layer2 = nn.Linear(256, 256)            # Hidden Layer 2: 256 -> 256 units\n",
    "        self.layer3 = nn.Linear(256, n_actions)      # Output Layer: 256 -> Action Q-values\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        Applies ReLU activation to hidden layers.\n",
    "        \"\"\"\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.layer1(x)) # Activation function: Rectified Linear Unit (ReLU)\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)      # Returns raw Q-values (no activation at output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3ceb8a-3b2b-4d42-8380-570fb372a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Hyperparameters Configuration\n",
    "# Defines key parameters controlling the Reinforcement Learning process.\n",
    "\n",
    "BATCH_SIZE = 128     # Number of transitions sampled from replay memory per update step\n",
    "GAMMA = 0.8          # Discount factor for future rewards (0.8 emphasizes near-term rewards)\n",
    "EPS_START = 0.9      # Initial Epsilon value for epsilon-greedy policy (High exploration)\n",
    "EPS_END = 0.05       # Minimum Epsilon value (Low exploration, high exploitation)\n",
    "EPS_DECAY = 500      # Rate of Epsilon decay (Slower decay due to large action space: 200 -> 500)\n",
    "TAU = 0.005          # Soft update rate for target network parameters\n",
    "LR = 0.001           # Learning Rate for the Adam optimizer\n",
    "num_episodes = 500   # Total number of training episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf6d79a-b3f8-49e7-b1f7-84db3565964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Environment Initialization\n",
    "# Sets up the quantum circuit environment and defines state/action dimensions.\n",
    "\n",
    "# Initialize the RL environment (Quantum Circuit Search)\n",
    "env = qc()\n",
    "\n",
    "# Define Action Space Size (Number of candidate gates)\n",
    "n_actions = env.act_space \n",
    "\n",
    "# Initialize Global Entanglement Weights (Transfer Learning context)\n",
    "# Set to None for the first episode to start from random initialization.\n",
    "weights_ent_global = None\n",
    "\n",
    "# Reset environment to starting state\n",
    "env.reset(weights_ent_global)\n",
    "\n",
    "# Define Observation Space Size\n",
    "# Flattens the observation matrix (Circuit Depth x Gate Features) into a 1D vector.\n",
    "# env.obs is a list of [Rot, CNOT, Param1, Param2], so length * 4 gives the total input dimension for DQN.\n",
    "n_observations = len(env.obs * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e87d09-1648-49ee-83dc-958380707906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Network Initialization and Optimizer Setup\n",
    "# Initializes the Policy Network and Target Network for the DQN algorithm.\n",
    "# The Target Network provides stable Q-value targets (Fixed Q-Targets) to prevent oscillation.\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict()) # Synchronize initial weights\n",
    "\n",
    "# Optimizer: AdamW (Adam with Weight Decay Fix) is used for parameter updates.\n",
    "# 'amsgrad=True' ensures long-term convergence properties.\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "# Initialize Experience Replay Buffer with a capacity of 10,000 transitions.\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0 # Global step counter for Epsilon decay scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a69735-8e55-4e7d-b680-bca1e73a99e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Action Selection Strategy (Epsilon-Greedy)\n",
    "# Balances exploration (random actions) and exploitation (best known actions) during training.\n",
    "\n",
    "def select_action(state):\n",
    "    \"\"\"\n",
    "    Selects an action based on the current state using an epsilon-greedy policy.\n",
    "    Args:\n",
    "        state: Current observation vector from the environment.\n",
    "    Returns:\n",
    "        tensor: Selected action index.\n",
    "    \"\"\"\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    \n",
    "    # Calculate Epsilon Threshold (Exponential Decay)\n",
    "    # Formula: epsilon = epsilon_end + (epsilon_start - epsilon_end) * exp(-steps / decay)\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    \n",
    "    steps_done += 1 # Increment global step counter\n",
    "    \n",
    "    # Exploitation: Select the action with the highest Q-value from the Policy Network\n",
    "    if sample > eps_threshold:\n",
    "        # print(\"select :\",steps_done) # Debugging log (Optional)\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) returns (max_value, index). We take [1] to get the index (action).\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "            \n",
    "    # Exploration: Select a random action from the action space\n",
    "    else:\n",
    "        return torch.tensor([[env.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200154ae-b0fc-4e82-a785-0146955ffecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Model Optimization Function\n",
    "# Performs a single step of Stochastic Gradient Descent (SGD) to update the Policy Network.\n",
    "# Utilizes Experience Replay and Fixed Q-Targets to stabilize training.\n",
    "\n",
    "def optimize_model():\n",
    "    \"\"\"\n",
    "    Optimizes the policy network by minimizing the loss between predicted Q-values and target Q-values.\n",
    "    Implements the Bellman Optimality Equation: Q(s,a) = r + gamma * max(Q(s',a'))\n",
    "    \"\"\"\n",
    "    # 1. Check Replay Memory Size\n",
    "    # Ensure there are enough samples to form a batch.\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    # 2. Sample a Batch of Transitions\n",
    "    # Randomly sample 'BATCH_SIZE' transitions to break temporal correlations.\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    # Converts batch-array of Transitions to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # 3. Prepare Tensor Batches\n",
    "    # Concatenate state, action, and reward tensors for batch processing.\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # 4. Compute Current Q-Values (Q(s_t, a_t))\n",
    "    # Forward pass through Policy Network and select Q-values corresponding to taken actions.\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    # 5. Compute Next State Values (V(s_{t+1}) = max_a Q(s_{t+1}, a))\n",
    "    # Initialize next state values to zero.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    \n",
    "    # Use Target Network to compute Q-values for next states (Fixed Q-Targets).\n",
    "    # We detach from the graph because target values are treated as constants (no gradient).\n",
    "    with torch.no_grad():\n",
    "        next_state_values = target_net(torch.cat(batch.next_state)).max(1)[0]\n",
    "\n",
    "    # 6. Compute Expected Q-Values (Bellman Target)\n",
    "    # Formula: y = r + gamma * max(Q_target(s', a'))\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # 7. Compute Loss\n",
    "    # Use Huber Loss (SmoothL1Loss) which is less sensitive to outliers than MSE.\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # 8. Backpropagation and Optimization\n",
    "    optimizer.zero_grad() # Clear previous gradients\n",
    "    loss.backward()       # Compute gradients\n",
    "    \n",
    "    # Gradient Clipping: Clamps gradients to [-100, 100] to prevent exploding gradients.\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    \n",
    "    optimizer.step()      # Update network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1774e06e-bdd8-4086-917a-a9361289147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Training Log Initialization\n",
    "# Initializes lists to store performance metrics and circuit data across all episodes.\n",
    "\n",
    "# Reward tracking\n",
    "reward_ep_list = []      # Detailed reward components per step\n",
    "reward_sum_ep_list = []  # Total cumulative reward per episode\n",
    "\n",
    "# Circuit and State tracking\n",
    "obs_ep_list = []         # History of observation vectors (circuit structures)\n",
    "outs_ep_list = []        # Optimization logs (cost/accuracy per iteration)\n",
    "figset_ep_list = []      # Data snapshots for visualization\n",
    "final_ep_list = []       # Final state summaries\n",
    "\n",
    "# Performance Metrics tracking (Accuracy & Cost)\n",
    "acc_train_last_ep = []\n",
    "acc_val_last_ep   = []\n",
    "acc_train_max_ep  = []\n",
    "acc_val_max_ep    = []\n",
    "cost_ep = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baefd799-d716-4d1a-b2f8-588ab128182c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 11: Main Reinforcement Learning Training Loop\n",
    "# Executes the episodic training process, handling agent-environment interaction,\n",
    "# memory storage, model optimization, and logging of performance metrics.\n",
    "\n",
    "%%time\n",
    "\n",
    "# Polyak averaging parameter for updating global entanglement weights (Transfer Learning)\n",
    "tau = 0.05\n",
    "\n",
    "# 1. Episode Loop\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    # Reset Environment and Initialize State\n",
    "    # Inherits global weights to maintain knowledge across episodes.\n",
    "    env.reset(weights_ent_global)\n",
    "    state = env.obs\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    # Initialize Episode-Specific Logs\n",
    "    reward_list = []; obs_list = []; outs_list = []; figset_list = []\n",
    "    \n",
    "    # 2. Time-Step Loop (Interaction within an episode)\n",
    "    for t in count():\n",
    "        \n",
    "        # A. Action Selection (Epsilon-Greedy)\n",
    "        action = select_action(state)\n",
    "        \n",
    "        # B. Environment Step\n",
    "        # Execute action, observe new state and reward\n",
    "        truncated = not env.step(action.item())\n",
    "\n",
    "        # Error Handling for invalid steps\n",
    "        if truncated:\n",
    "            print('truncated error')\n",
    "            break\n",
    "        \n",
    "        observation = env.obs\n",
    "        reward = env.reward\n",
    "        terminated = env.term\n",
    "        done = env.done\n",
    "\n",
    "        # Format Tensors for Replay Memory\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        # C. Store Transition in Experience Replay Buffer\n",
    "        memory.push(state, action, next_state, reward, done)\n",
    "        state = next_state\n",
    "\n",
    "        # D. Optimization Step (DQN Update)\n",
    "        # Sample batch and perform backpropagation\n",
    "        optimize_model()\n",
    "\n",
    "        # E. Target Network Soft Update (Polyak Averaging)\n",
    "        # target_params = TAU * policy_params + (1 - TAU) * target_params\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        \n",
    "        # F. Logging (Per step)\n",
    "        reward_list.append(float(reward))\n",
    "        obs_list.append(env.draw)\n",
    "        outs_list.append(env.outs)\n",
    "        figset_list.append(env.figset)\n",
    "        \n",
    "        # 3. Episode Termination Handling\n",
    "        if terminated:\n",
    "            # Extract Performance Metrics from the last optimization step\n",
    "            cost_series = [row[1] for row in env.outs]\n",
    "            acc_train_series = [row[2] for row in env.outs]\n",
    "            acc_val_series   = [row[3] for row in env.outs]\n",
    "\n",
    "            acc_train_last = float(acc_train_series[-1])\n",
    "            acc_val_last   = float(acc_val_series[-1])\n",
    "            acc_train_max  = float(max(acc_train_series))\n",
    "            acc_val_max    = float(max(acc_val_series))\n",
    "            cost_val = float(cost_series[-1])\n",
    "\n",
    "            # Update Global Training Logs\n",
    "            acc_train_last_ep.append(acc_train_last)\n",
    "            acc_val_last_ep.append(acc_val_last)\n",
    "            acc_train_max_ep.append(acc_train_max)\n",
    "            acc_val_max_ep.append(acc_val_max)\n",
    "            cost_ep.append(cost_val)\n",
    "\n",
    "            # Record Best Performance in this Episode\n",
    "            max_value = max(reward_list)\n",
    "            max_index = reward_list.index(max_value)\n",
    "            reward_ep_list.append(max_value)\n",
    "            reward_sum_ep_list.append(sum(reward_list))\n",
    "            obs_ep_list.append(obs_list[max_index])\n",
    "            outs_ep_list.append(outs_list[max_index])\n",
    "            # Save the circuit structure and data snapshot of the best step\n",
    "            figset_ep_list.append([figset_list[max_index], env.gatestream[:max_index+1]])\n",
    "            final_ep_list.append([figset_list[-1], env.gatestream[:t+1]])\n",
    "\n",
    "            # G. Global Weights Soft Update (Transfer Learning)\n",
    "            # Updates the shared entanglement weights based on the current episode's result.\n",
    "            if env.weights_ent is not None:\n",
    "                if weights_ent_global is None:\n",
    "                    weights_ent_global = env.weights_ent.detach().clone()\n",
    "                else:\n",
    "                    weights_ent_global = (1 - tau) * weights_ent_global + tau * env.weights_ent.detach()\n",
    "\n",
    "            # H. Final Episode Artifact Generation\n",
    "            if i_episode == num_episodes - 1:\n",
    "                # Generate final circuit diagram\n",
    "                outs_final, draw_p, _, _ = opt_classifier(\n",
    "                    env.gatestream,\n",
    "                    weights_ent=env.weights_ent,\n",
    "                    iters=0, draw=True\n",
    "                )\n",
    "                if draw_p is not None:\n",
    "                    print(f\"\\n=== Final circuit (episode #{i_episode+1}, steps={t+1}) ===\")\n",
    "                    print(draw_p)\n",
    "                    # Save circuit diagram to file\n",
    "                    with open(\"final_circuit.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(draw_p)\n",
    "\n",
    "                # Save circuit architecture (JSON) and weights (PT)\n",
    "                with open(\"final_gatestream.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(env.gatestream, f, ensure_ascii=False, indent=2)\n",
    "                torch.save(env.weights_ent.detach().cpu(), \"final_weights_ent.pt\")\n",
    "                    \n",
    "            # Print Progress Log\n",
    "            print(f\"Episode complete : {i_episode+1} ({t+1}) | cost={cost_val:.3f}, acc_val_last={acc_val_last:.3f}, acc_val_max={acc_val_max:.3f}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b80574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Training Performance Visualization (Cumulative Reward)\n",
    "# Plots the total reward accumulated per episode to analyze the agent's learning convergence.\n",
    "# An upward trend indicates that the agent is successfully learning to construct better circuits.\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward sum')\n",
    "plt.plot(reward_sum_ep_list) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8469ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Training Performance Visualization (Maximum Reward)\n",
    "# Plots the maximum reward achieved in each episode.\n",
    "# This metric helps identify if the agent is discovering high-quality solutions (peak performance),\n",
    "# even if the average performance fluctuates due to exploration.\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Max Reward')\n",
    "plt.plot(reward_ep_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b6651a-0634-45d0-8a14-0dc6d3a8efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Data Persistence (Saving Experiment Results)\n",
    "# Serializes and saves all training metrics, circuit structures, and performance logs to disk.\n",
    "# This ensures that the experimental results are preserved for post-training analysis and visualization.\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save Reward Metrics (Training Trajectory)\n",
    "with open('reward_ep_list_0121.pkl', 'wb') as f: pickle.dump(reward_ep_list, f)\n",
    "with open('reward_sum_ep_list_0121.pkl', 'wb') as f: pickle.dump(reward_sum_ep_list, f)\n",
    "\n",
    "# Save Circuit Architectures and Optimization Logs\n",
    "with open('obs_ep_list_0121.pkl', 'wb') as f: pickle.dump(obs_ep_list, f)\n",
    "with open('outs_ep_list_0121.pkl', 'wb') as f: pickle.dump(outs_ep_list, f)\n",
    "\n",
    "# Save Visualization Data and Final Model States\n",
    "with open('figset_ep_list_0121.pkl', 'wb') as f: pickle.dump(figset_ep_list, f)\n",
    "with open('final_ep_list_0121.pkl', 'wb') as f: pickle.dump(final_ep_list, f)\n",
    "\n",
    "# Save Detailed Performance Metrics (Accuracy and Cost)\n",
    "with open('acc_val_max_ep_0121.pkl', 'wb') as f: pickle.dump(acc_val_max_ep, f)\n",
    "with open('acc_val_last_ep_0121.pkl', 'wb') as f: pickle.dump(acc_val_last_ep, f)\n",
    "with open('cost_ep_0121.pkl', 'wb') as f: pickle.dump(cost_ep, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
