{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a971b6bd-869a-45a1-9cf9-9b611b5551b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Environment Setup and Configuration (Model C: Pure RL)\n",
    "# Sets up the computational environment for the Pure Reinforcement Learning model.\n",
    "# Note: Unlike Model B, this configuration excludes the fixed Strongly Entangling Layers (SEL),\n",
    "# focusing solely on the adaptive circuit structure generated by the RL agent.\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# Check for CUDA availability to enable GPU acceleration\n",
    "torch_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Torch CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185a30a4-afc7-461f-8b18-33a161fd6457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Quantum Device Initialization\n",
    "# Configures the quantum simulator. 'lightning.qubit' is a high-performance C++ backend.\n",
    "# This device will execute the variational circuits constructed by the RL agent.\n",
    "\n",
    "dev = qml.device(\"lightning.qubit\", wires=6, shots=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafce97e-f8cc-4b54-a0f2-4dfea4ee303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Parameter Counting Utility\n",
    "# Calculates the number of trainable parameters required for the dynamically generated circuit.\n",
    "# Iterates through the gate sequence to count parameterized gates (e.g., 'Rot').\n",
    "# Note: In Model C (Pure RL), this count represents the total number of trainable parameters in the circuit,\n",
    "# as there are no pre-defined Strongly Entangling Layers.\n",
    "\n",
    "def check_np(list):\n",
    "    \"\"\"\n",
    "    Counts the number of parameterized gates in the generated circuit architecture.\n",
    "    Args:\n",
    "        list: The list of gates (operations) defining the circuit.\n",
    "    Returns:\n",
    "        int: The total count of gates requiring optimization parameters.\n",
    "    \"\"\"\n",
    "    num = 0\n",
    "    for r in list:\n",
    "        # Check if the gate is a Rotation gate ('Rot') which requires parameters.\n",
    "        if r[0] == \"Rot\": \n",
    "            num += 1\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa70b326-5691-4111-b9de-c5adb44eac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Dynamic Ansatz Construction (RL-driven)\n",
    "# Constructs the variational circuit based solely on the gate sequence (gatestream) provided by the RL agent.\n",
    "# In this Pure RL model (Model C), this function defines the entire trainable unitary evolution.\n",
    "\n",
    "def ansatz(W, gatestream):\n",
    "    \"\"\"\n",
    "    Builds the quantum circuit dynamically.\n",
    "    Args:\n",
    "        W: Trainable parameters for rotation gates.\n",
    "        gatestream: List of gates (Action history from RL agent).\n",
    "    \"\"\"\n",
    "    w_cnt = 0\n",
    "    for gate in gatestream:\n",
    "        # If the gate is a parameterized rotation\n",
    "        if gate[0] == \"Rot\":\n",
    "            # Apply Pauli Rotation: Param W[w_cnt], Axis gate[1], Target wire gate[2]\n",
    "            qml.PauliRot(W[w_cnt], gate[1], wires=gate[2])\n",
    "            w_cnt += 1\n",
    "\n",
    "        # If the gate is CNOT (Entanglement)\n",
    "        elif gate[0] == \"CNOT\":\n",
    "            qml.CNOT(wires=[gate[1], gate[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9e349-edc0-4671-902c-7d4d35a1a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: State Preparation (Amplitude Embedding)\n",
    "# Maps classical input vectors into the quantum Hilbert space.\n",
    "\n",
    "def statepreparation(x):\n",
    "    \"\"\"\n",
    "    Encodes the input feature vector x into quantum state amplitudes.\n",
    "    Args:\n",
    "        x (tensor): Normalized input feature vector.\n",
    "    \"\"\"\n",
    "    # Amplitude Embedding: Encodes N features into log2(N) qubits.\n",
    "    # Normalization ensures the input vector represents a valid quantum state.\n",
    "    qml.AmplitudeEmbedding(x, wires=range(6), normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d31402-5d6d-4607-8493-ec6f875b98e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Quantum Node (QNode) Definition\n",
    "# Defines the quantum circuit execution pipeline using the 'adjoint' differentiation method.\n",
    "# In Model C (Pure RL), the circuit architecture is entirely determined by the RL agent (gatestream),\n",
    "# without any pre-fixed Strongly Entangling Layers.\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\", diff_method=\"adjoint\")\n",
    "def circuit(weights_rl, x, gatestream):\n",
    "    \"\"\"\n",
    "    Executes the variational quantum circuit.\n",
    "    Args:\n",
    "        weights_rl (tensor): Trainable parameters for the RL-generated rotation gates.\n",
    "        x (tensor): Input feature vector.\n",
    "        gatestream (list): The sequence of gates defined by the RL agent.\n",
    "    Returns:\n",
    "        float: Expectation value of the Pauli-Z operator on the first qubit.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. State Preparation (Data Embedding)\n",
    "    statepreparation(x)\n",
    "    \n",
    "    # 2. Variational Circuit (RL-driven Ansatz)\n",
    "    # Applies the dynamic circuit structure optimized by the RL agent.\n",
    "    # Note: Unlike Model B, 'weights_ent' is not used, and no fixed entanglement layers are applied.\n",
    "    ansatz(weights_rl, gatestream)\n",
    "\n",
    "    # 3. Measurement\n",
    "    # Computes the expectation value <Z> as the model prediction (Range: [-1, 1]).\n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee6039f-df2f-44b5-b51b-a1a85d7d07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Variational Quantum Classifier (VQC) Model\n",
    "# Combines the quantum circuit output with a classical bias term to form the final prediction model.\n",
    "\n",
    "def variational_classifier(weights_rl, bias, x, gatestream):\n",
    "    \"\"\"\n",
    "    Computes the classifier output for a given input x.\n",
    "    Args:\n",
    "        weights_rl (tensor): Trainable parameters for the RL-generated circuit.\n",
    "        bias (tensor): Classical bias term to shift the decision boundary.\n",
    "        x (tensor): Input data vector.\n",
    "        gatestream (list): Circuit architecture defined by the RL agent.\n",
    "    Returns:\n",
    "        tensor: The raw prediction score (logit) before thresholding.\n",
    "    \"\"\"\n",
    "    # The model output is the expectation value of the quantum circuit plus a classical bias.\n",
    "    # y_pred = <Z> + b\n",
    "    return circuit(weights_rl, x, gatestream) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e2408-a9eb-4244-9b58-c114e6e6f3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Mean Squared Error (MSE) Loss\n",
    "# Standard loss function for regression-based quantum classification.\n",
    "\n",
    "def square_loss(labels, predictions):\n",
    "    \"\"\"\n",
    "    Computes the Mean Squared Error between labels and predictions.\n",
    "    Includes tensor type/device compatibility checks.\n",
    "    \"\"\"\n",
    "    # Ensure predictions are a tensor\n",
    "    preds  = torch.stack(predictions).squeeze() if isinstance(predictions, list) else predictions\n",
    "    # Match label tensor properties to predictions\n",
    "    labels = torch.as_tensor(labels, dtype=preds.dtype, device=preds.device).squeeze()\n",
    "    \n",
    "    # Calculate Mean Squared Error\n",
    "    return ((labels - preds) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e94555-4060-4d6f-b96e-ab319febcf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Classification Accuracy (Sign-based)\n",
    "# Measures accuracy by comparing the sign of predictions with true labels {-1, 1}.\n",
    "\n",
    "def accuracy_sign(labels, predictions):\n",
    "    \"\"\"\n",
    "    Computes accuracy for binary classification.\n",
    "    Method: Sign thresholding (pred >= 0 -> Class 1, else -> Class -1).\n",
    "    Args:\n",
    "        labels: True labels tensor.\n",
    "        predictions: Model output tensor (expectation values).\n",
    "    \"\"\"\n",
    "    # Ensure tensor compatibility\n",
    "    preds  = torch.stack(predictions).squeeze() if isinstance(predictions, list) else predictions\n",
    "    labels = labels.to(dtype=preds.dtype, device=preds.device).squeeze()\n",
    "    \n",
    "    # Convert continuous predictions to discrete class labels {1.0, -1.0}\n",
    "    pred_labels = torch.where(preds >= 0, 1.0, -1.0)\n",
    "    \n",
    "    # Calculate mean accuracy\n",
    "    return (pred_labels == labels).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f49c77-4c91-47be-8af5-19cec5c0560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Cost Function Calculation\n",
    "# Computes the total cost (loss) over a batch of data by aggregating individual predictions.\n",
    "# In Model C, the optimization is performed solely on the RL-generated circuit parameters (weights_rl).\n",
    "\n",
    "def cost(weights_rl, bias, X, Y, gatestream):\n",
    "    \"\"\"\n",
    "    Calculates the loss for a given dataset batch.\n",
    "    Args:\n",
    "        weights_rl (tensor): Trainable parameters for the RL-generated circuit.\n",
    "        bias (tensor): Classical bias term.\n",
    "        X (tensor): Batch of input data.\n",
    "        Y (tensor): Batch of true labels.\n",
    "        gatestream (list): Circuit structure defined by the RL agent.\n",
    "    Returns:\n",
    "        tensor: Scalar loss value used for gradient descent.\n",
    "    \"\"\"\n",
    "    # Generate predictions for each sample in the batch\n",
    "    # Note: 'variational_classifier' is called without 'weights_ent' in this Pure RL model.\n",
    "    predictions = [variational_classifier(weights_rl, bias, x, gatestream) for x in X]\n",
    "    \n",
    "    # Calculate Square Loss (MSE) between predictions and ground truth\n",
    "    return square_loss(Y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a8ad46-cbea-443f-acca-53397c562712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Optimization Routine (Local Training)\n",
    "# Trains the parameters of the specific circuit architecture generated by the RL agent.\n",
    "# This function acts as the \"inner loop\" of the reinforcement learning process,\n",
    "# providing the reward signal (accuracy) based on the circuit's performance.\n",
    "# Note: In Model C, optimization is applied solely to the RL-generated parameters (weights_rl) and bias.\n",
    "\n",
    "def opt_classifier(gatestream, iters=15, draw=False):\n",
    "    \"\"\"\n",
    "    Optimizes the parameters of the quantum circuit for a fixed structure.\n",
    "    Args:\n",
    "        gatestream (list): The circuit architecture defined by the RL agent.\n",
    "        iters (int): Number of optimization iterations.\n",
    "        draw (bool): Whether to generate a circuit diagram after training.\n",
    "    Returns:\n",
    "        tuple: Optimization logs, circuit diagram string, and final model state.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # 1. Data Loading and Preprocessing\n",
    "    # Load the Speck dataset (features and labels).\n",
    "    X = torch.from_numpy(np.load(os.path.join(\"dataset\", \"data_speck.npy\"))).float()\n",
    "    Y = torch.from_numpy(np.load(os.path.join(\"dataset\", \"labels_speck.npy\"))).float()\n",
    "    Y = Y * 2 - 1 # Rescale labels from [0, 1] to [-1, 1] for hinge-like loss compatibility.\n",
    "\n",
    "    # 2. Data Splitting\n",
    "    np.random.seed(0) # Fix seed for reproducibility\n",
    "    \n",
    "    num_data = len(Y)\n",
    "    num_train = int(0.8 * num_data) # 80% Training, 20% Validation\n",
    "    indices = torch.randperm(num_data, device=Y.device) \n",
    "    X_train = X[indices[:num_train]]\n",
    "    Y_train = Y[indices[:num_train]]\n",
    "    X_val   = X[indices[num_train:]]\n",
    "    Y_val   = Y[indices[num_train:]]\n",
    "\n",
    "    dev_t = X.device\n",
    "\n",
    "    # 3. Parameter Initialization\n",
    "    # Count the number of required parameters for the RL-generated circuit.\n",
    "    num_rot_params = check_np(gatestream)\n",
    "    \n",
    "    if num_rot_params > 0:\n",
    "        # Initialize weights with small random values to break symmetry.\n",
    "        weights_rl = (torch.randn(num_rot_params, device=dev_t) * 0.01).requires_grad_(True)\n",
    "    else:\n",
    "        # Handle case where no parameterized gates exist in the circuit.\n",
    "        weights_rl = torch.zeros(0, device=dev_t, requires_grad=True)\n",
    "\n",
    "    # Initialize classical bias term\n",
    "    bias = torch.tensor(0.0, device=dev_t, requires_grad=True)\n",
    "\n",
    "    # Define Optimizer: Stochastic Gradient Descent (SGD) with Momentum.\n",
    "    # Note: Only 'weights_rl' and 'bias' are optimized.\n",
    "    opt = torch.optim.SGD([weights_rl, bias], lr=0.01, momentum=0.9)\n",
    "    batch_size = 10\n",
    "\n",
    "    out_list = []\n",
    "\n",
    "    # 4. Optimization Loop\n",
    "    for it in range(iters):\n",
    "        \n",
    "        # Mini-batch sampling\n",
    "        batch_idx = torch.randint(0, num_train, (batch_size,), device=dev_t)\n",
    "        X_batch, Y_batch = X_train[batch_idx], Y_train[batch_idx]\n",
    "\n",
    "        # Backpropagation step\n",
    "        opt.zero_grad()\n",
    "        # Compute loss (Model C: cost function does not take weights_ent)\n",
    "        loss = cost(weights_rl, bias, X_batch, Y_batch, gatestream)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # 5. Evaluation and Logging\n",
    "        # Compute performance metrics without updating gradients.\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Training Metrics\n",
    "            vals_tr  = [variational_classifier(weights_rl, bias, x, gatestream) for x in X_train]\n",
    "            preds_tr = torch.stack(vals_tr).squeeze()\n",
    "            acc_train = (torch.sign(preds_tr) == torch.sign(Y_train)).float().mean()\n",
    "\n",
    "            # Validation Metrics\n",
    "            vals_val  = [variational_classifier(weights_rl, bias, x, gatestream) for x in X_val]\n",
    "            preds_val = torch.stack(vals_val).squeeze()\n",
    "            acc_val   = (torch.sign(preds_val) == torch.sign(Y_val)).float().mean()\n",
    "            cost_val  = ((Y_val - preds_val) ** 2).mean()\n",
    "\n",
    "        out_list.append([it + 1, float(cost_val), float(acc_train), float(acc_val)])\n",
    "\n",
    "        # Early Stopping: Terminate if validation accuracy reaches the threshold (0.65).\n",
    "        if float(acc_val) >= 0.65:\n",
    "            break\n",
    "\n",
    "    # 6. Circuit Visualization\n",
    "    if draw and len(X_val) > 0:\n",
    "        x_draw     = X_val[0].detach().cpu().numpy()\n",
    "        w_rl_draw  = weights_rl.detach().cpu().numpy()\n",
    "        # Draw the circuit structure using PennyLane's drawer\n",
    "        draw_p = qml.draw(circuit)(w_rl_draw, x_draw, gatestream)\n",
    "    else:\n",
    "        draw_p = None    \n",
    "\n",
    "    # Bundle final state for return\n",
    "    figset = [weights_rl.detach(), bias.detach(), X_train, Y_train, X_val, Y_val]\n",
    "    \n",
    "    return out_list, draw_p, figset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50641228-1aef-4d19-8266-26d4b24c1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Gate Encoding Utility (Symbolic to Numerical)\n",
    "# Converts discrete gate definitions into numerical vectors for the RL agent's state representation.\n",
    "# This encoding facilitates the neural network's processing of circuit structures.\n",
    "\n",
    "def gate_to_obs(gate):\n",
    "    \"\"\"\n",
    "    Transforms a gate specification into a fixed-size numerical observation vector.\n",
    "    Encoding Schema: [Is_Rot, Is_CNOT, Parameter_1 (Axis/Control), Parameter_2 (Target)]\n",
    "    Args:\n",
    "        gate (list): Symbolic gate representation (e.g., ['Rot', 'X', 0]).\n",
    "    Returns:\n",
    "        list: Numerical vector representation (e.g., [1, 0, 1, 0]).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize observation vector [Type_Rot, Type_CNOT, Param1, Param2]\n",
    "    ob = [0, 0, 0, 0]\n",
    "    \n",
    "    if gate[0] == 'Rot':\n",
    "        ob[0] = 1 # Set Rotation Flag\n",
    "        \n",
    "        # Encode Rotation Axis: X=1, Y=2, Z=3\n",
    "        if gate[1] == 'X': ob[2] = 1\n",
    "        elif gate[1] == 'Y': ob[2] = 2\n",
    "        elif gate[1] == 'Z': ob[2] = 3\n",
    "        \n",
    "        ob[3] = gate[2] # Target Qubit Index\n",
    "    \n",
    "    elif gate[0] == 'CNOT':\n",
    "        ob[1] = 1 # Set CNOT Flag\n",
    "        ob[2] = gate[1] # Control Qubit Index\n",
    "        ob[3] = gate[2] # Target Qubit Index\n",
    "    \n",
    "    return ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36dceb4-ea56-4921-9a30-433cd0dd0885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: State Update Mechanism\n",
    "# Updates the environment's internal state and observation vector based on the agent's selected action.\n",
    "# This function physically adds the chosen gate to the circuit architecture.\n",
    "\n",
    "def update_obs(act, step, obs, gatestream, gates):\n",
    "    \"\"\"\n",
    "    Applies the selected action to the environment.\n",
    "    Args:\n",
    "        act (int): Index of the selected action (gate).\n",
    "        step (int): Current time step (depth of the circuit).\n",
    "        obs (list): Current observation vector (history of gates).\n",
    "        gatestream (list): Current sequence of gates defining the circuit.\n",
    "        gates (list): List of all possible gates (Action Space).\n",
    "    Returns:\n",
    "        tuple: Updated step count, observation vector, and gatestream.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Circuit Construction\n",
    "    # Append the selected gate to the growing circuit sequence.\n",
    "    gatestream.append(gates[act])\n",
    "    \n",
    "    # 2. Observation Update\n",
    "    # Convert the symbolic gate to its numerical representation.\n",
    "    ob = gate_to_obs(gates[act])\n",
    "    # Update the observation vector at the current time step.\n",
    "    obs[step] = ob\n",
    "    \n",
    "    # 3. Increment Time Step\n",
    "    step += 1\n",
    "\n",
    "    return step, obs, gatestream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134f7005-29a4-4b1c-b9bb-a0a299a37961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Reward Function Definition (Multi-objective)\n",
    "# Calculates the scalar reward signal for the RL agent based on circuit performance and structural heuristics.\n",
    "# The reward acts as the fitness function, guiding the agent towards circuits that are both\n",
    "# accurate (high classification performance) and efficient (low depth, uniform gate distribution).\n",
    "\n",
    "def cal_reward(steps, obs, outs):\n",
    "    \"\"\"\n",
    "    Computes the reward for the current step.\n",
    "    Args:\n",
    "        steps (int): Current depth of the circuit (number of gates added).\n",
    "        obs (list): History of gates (observation vectors).\n",
    "        outs (list): Optimization logs from 'opt_classifier' [[iter, cost, acc_train, acc_val], ...].\n",
    "    Returns:\n",
    "        tuple: (List of individual reward components, Weighted Total Scalar Reward)\n",
    "    \"\"\"\n",
    "    \n",
    "    ## 1. Performance-based Rewards\n",
    "    # Accuracy: Average training accuracy over the optimization trajectory.\n",
    "    acc = [row[2] for row in outs]\n",
    "    acc_m = sum(acc) / len(acc)\n",
    "\n",
    "    # Cost: Inverse of the average loss (Lower loss -> Higher reward).\n",
    "    cost = [row[1] for row in outs]\n",
    "    cost_m = 1 / (sum(cost) / len(cost))\n",
    "\n",
    "    ## 2. Structural Uniformity Reward (Variance)\n",
    "    # Encourages the agent to distribute gates evenly across all qubits, preventing bottlenecks.\n",
    "    pop_list = [0, 0, 0, 0, 0, 0] # Counter for gate usage per qubit\n",
    "    for row in obs:\n",
    "        if row[1] == 1: # CNOT gate involves two qubits\n",
    "            pop_list[row[2]] += 1\n",
    "            pop_list[row[3]] += 1\n",
    "        elif row[0] == 1: # Rotation gate involves one qubit\n",
    "            pop_list[row[3]] += 1\n",
    "    # Reward is higher if variance of gate counts is lower.\n",
    "    pop_r = (2 - np.var(pop_list)) / 2    \n",
    "\n",
    "    ## 3. Redundancy Penalty (Duplicate Check)\n",
    "    # Penalizes the agent for applying the exact same gate consecutively on the same qubits.\n",
    "    # This prevents useless operations like Rot(x) followed immediately by Rot(x).\n",
    "    dup_r = 0\n",
    "    if obs[steps-1][0] == 1: # Check Rotation duplicates\n",
    "        tc = obs[steps-1][3]\n",
    "        tc_list = []\n",
    "        for row in obs:\n",
    "            if row[1] == 1:\n",
    "                if row[2] == tc or row[3] == tc: tc_list.append(row)\n",
    "            elif row[0] == 1:\n",
    "                if row[3] == tc: tc_list.append(row)\n",
    "        if len(tc_list) > 1:\n",
    "            if tc_list[-1] == tc_list[-2]: dup_r = -10 # Heavy penalty for duplicates\n",
    "            \n",
    "    elif obs[steps-1][1] == 1: # Check CNOT duplicates\n",
    "        # Verify both Control and Target qubit histories\n",
    "        tc = obs[steps-1][2]\n",
    "        tc_list_c = []\n",
    "        for row in obs:\n",
    "            if row[1] == 1:\n",
    "                if row[2] == tc or row[3] == tc: tc_list_c.append(row)\n",
    "            elif row[0] == 1:\n",
    "                if row[3] == tc: tc_list_c.append(row)\n",
    "        tc = obs[steps-1][3]\n",
    "        tc_list_t = []\n",
    "        for row in obs:\n",
    "            if row[1] == 1:\n",
    "                if row[2] == tc or row[3] == tc: tc_list_t.append(row)\n",
    "            elif row[0] == 1:\n",
    "                if row[3] == tc: tc_list_t.append(row)\n",
    "        if len(tc_list_c) > 1 and len(tc_list_t) > 1:\n",
    "            if tc_list_c[-1] == tc_list_c[-2] and tc_list_t[-1] == tc_list_t[-2]: dup_r = -10\n",
    "\n",
    "    ## 4. Gate Type Bias\n",
    "    # Encourages the use of Parameterized Rotation gates (which provide trainability).\n",
    "    if obs[steps-1][0] == 1:\n",
    "        gate_r = 1\n",
    "        rot_r = 1\n",
    "    else:\n",
    "        gate_r = 0\n",
    "        rot_r = 0\n",
    "\n",
    "    ## 5. Topology Constraint (CNOT Distance)\n",
    "    # Rewards CNOT gates between physically adjacent or close qubits (Locality).\n",
    "    if obs[steps-1][1] == 1:   \n",
    "        cnot_r = 1 / abs(obs[steps-1][2]-obs[steps-1][3])\n",
    "    else: cnot_r = 0    \n",
    "\n",
    "    ## 6. Efficiency Reward (Circuit Depth)\n",
    "    # Encourages shorter circuits by penalizing the number of steps.\n",
    "    steps_r = (60 - steps) / 60\n",
    "    \n",
    "    ## Total Weighted Reward Calculation\n",
    "    # Combines all components with specific hyperparameters.\n",
    "    # Note: Accuracy has the highest weight (x30 effectively), prioritizing valid classification.\n",
    "    return [acc_m, cost_m, gate_r, rot_r, cnot_r, steps_r, pop_r, dup_r], (acc_m - 0.5)*2 * 15 + cost_m * 2 + gate_r * 3 + rot_r + cnot_r + steps_r * 5 + pop_r * 3 + dup_r  ## with weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1558c-0e6a-4890-b20a-a6d7072dd28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Quantum Circuit Environment Class (OpenAI Gym-like Interface)\n",
    "# Encapsulates the quantum circuit generation process as a Reinforcement Learning environment.\n",
    "# Defines the Action Space, State Space (Observation), and Transition Logic.\n",
    "\n",
    "class qc:\n",
    "    \"\"\"\n",
    "    The environment class for the RL agent.\n",
    "    Manages the state (current circuit structure), executes actions (adding gates),\n",
    "    and calculates rewards based on the performance of the generated circuit.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Define Action Space: All possible gates the agent can select.\n",
    "        # Includes parameterized rotations (Rot) on all axes (X,Y,Z) for all qubits,\n",
    "        # and entangling gates (CNOT) for all possible pairs.\n",
    "        self.gates = [['Rot','X', 0], ['Rot','X', 1], ['Rot','X', 2], ['Rot','X', 3], ['Rot','X', 4], ['Rot','X', 5],\n",
    "         ['Rot','Y', 0], ['Rot','Y', 1], ['Rot','Y', 2], ['Rot','Y', 3], ['Rot','Y', 4], ['Rot','Y', 5],\n",
    "         ['Rot','Z', 0], ['Rot','Z', 1], ['Rot','Z', 2], ['Rot','Z', 3], ['Rot','Z', 4], ['Rot','Z', 5],\n",
    "         ['CNOT', 0, 1],  ['CNOT', 0, 2],  ['CNOT', 0, 3], ['CNOT', 0, 4], ['CNOT', 0, 5],\n",
    "         ['CNOT', 1, 0],  ['CNOT', 1, 2],  ['CNOT', 1, 3], ['CNOT', 1, 4], ['CNOT', 1, 5],\n",
    "         ['CNOT', 2, 0],  ['CNOT', 2, 1],  ['CNOT', 2, 3], ['CNOT', 2, 4], ['CNOT', 2, 5],\n",
    "         ['CNOT', 3, 0],  ['CNOT', 3, 1],  ['CNOT', 3, 2], ['CNOT', 3, 4], ['CNOT', 3, 5],\n",
    "         ['CNOT', 4, 0],  ['CNOT', 4, 1],  ['CNOT', 4, 2], ['CNOT', 4, 3], ['CNOT', 4, 5],\n",
    "         ['CNOT', 5, 0],  ['CNOT', 5, 1],  ['CNOT', 5, 2], ['CNOT', 5, 3], ['CNOT', 5, 4]] \n",
    "        \n",
    "        self.len_qc = 60 # Maximum circuit depth (time steps per episode)\n",
    "        self.act_space = len(self.gates) # Size of the action space\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment to the initial state.\n",
    "        Note: Unlike Model B, this method does not accept 'weights_ent' as input,\n",
    "        confirming that no pre-trained entanglement layers are used. The agent starts with an empty circuit.\n",
    "        \"\"\"\n",
    "        self.steps = 0\n",
    "        self.obs = [[0] * 4 for _ in range(self.len_qc)] # Initialize empty observation grid\n",
    "        self.gatestream = [] # Clear gate history\n",
    "        self.reward = -1\n",
    "        self.term = -1\n",
    "        self.done = 0\n",
    "        return\n",
    "\n",
    "    def step(self, act):\n",
    "        \"\"\"\n",
    "        Executes one time step within the environment.\n",
    "        1. Updates the circuit structure with the chosen gate (action).\n",
    "        2. Trains the current circuit parameters (weights_rl only) via 'opt_classifier'.\n",
    "        3. Computes the reward based on accuracy, cost, and structural heuristics.\n",
    "        \"\"\"\n",
    "        # Validity checks\n",
    "        if act > self.act_space-1 or act < 0:\n",
    "            print(\"out of action space\")\n",
    "            return 0\n",
    "        if self.steps > self.len_qc-1:\n",
    "            print(\"out of qc length\")\n",
    "            return 0\n",
    "        \n",
    "        # 1. State Transition\n",
    "        # Apply the action to update the observation and gatestream.\n",
    "        self.steps, self.obs, self.gatestream = update_obs(act, self.steps, self.obs, self.gatestream, self.gates)\n",
    "        \n",
    "        # 2. Performance Evaluation (Inner Loop Optimization)\n",
    "        # Train the current circuit structure to determine its potential performance.\n",
    "        # Note: Optimization targets only the RL-generated parameters.\n",
    "        self.outs, self.draw, self.figset = opt_classifier(\n",
    "            self.gatestream,\n",
    "            iters=15, draw=False\n",
    "        )\n",
    "        \n",
    "        # 3. Reward Calculation\n",
    "        self.rlist, self.reward = cal_reward(self.steps, self.obs, self.outs)\n",
    "\n",
    "        # 4. Termination Check (Max Steps)\n",
    "        if self.steps == self.len_qc or self.done == 1:\n",
    "            self.term = 1\n",
    "        else: self.term = 0\n",
    "\n",
    "        # 5. Success Check (Accuracy Threshold)\n",
    "        # If the circuit achieves sufficient accuracy (>= 0.65), terminate early.\n",
    "        acc_val_max   = max(row[3] for row in self.outs)\n",
    "        if acc_val_max >= 0.65:  \n",
    "            self.done = 1\n",
    "            self.term = 1\n",
    "        \n",
    "        return 1\n",
    "\n",
    "    def gs_step(self, gs):\n",
    "        \"\"\"\n",
    "        Helper function to evaluate a specific gate sequence (gatestream) manually.\n",
    "        \"\"\"\n",
    "        self.outs, self.draw, self.figset = opt_classifier(\n",
    "            gs, iters=15, draw=False\n",
    "        )\n",
    "        return 1\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Selects a random action from the action space (Exploration).\n",
    "        \"\"\"\n",
    "        return random.randint(0, self.act_space-1)\n",
    "    \n",
    "    def showdb(self, figset, gatestream):\n",
    "        \"\"\"\n",
    "        Visualizes the decision boundary of the trained model using PCA.\n",
    "        Args:\n",
    "            figset: Contains trained weights and dataset splits.\n",
    "            gatestream: The circuit structure used for classification.\n",
    "        \"\"\"\n",
    "        weights_rl, bias, X_train, Y_train, X_val, Y_val = figset\n",
    "        \n",
    "        X_all = torch.cat([X_train, X_val], dim=0)\n",
    "        Y_all = torch.cat([Y_train, Y_val], dim=0)\n",
    "        split = len(X_train)\n",
    "\n",
    "        # Generate predictions using the Pure RL model (no weights_ent)\n",
    "        preds = [torch.sign(variational_classifier(weights_rl, bias, x, gatestream)) for x in X_all]\n",
    "        preds = torch.stack(preds).squeeze().cpu().numpy()\n",
    "\n",
    "        # PCA Projection for 2D Visualization\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        X_2d = pca.fit_transform(X_all.cpu().numpy())\n",
    "\n",
    "        # Determine Correct/Wrong predictions\n",
    "        correct = (preds == Y_all.cpu().numpy())\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.scatter(X_2d[correct, 0], X_2d[correct, 1], c=\"g\", label=\"Correct\", alpha=0.7)\n",
    "        plt.scatter(X_2d[~correct, 0], X_2d[~correct, 1], c=\"r\", label=\"Wrong\", alpha=0.7, marker=\"x\")\n",
    "    \n",
    "        # Draw vertical line separating Train (left) and Val (right) implicitly by index if sorted, \n",
    "        # or just as a visual marker for the split point in the array.\n",
    "        plt.axvline(X_2d[split, 0], color=\"gray\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "        plt.legend()\n",
    "        plt.title(\"Classification results (PCA 2D projection)\")\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
