{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baa9ff6-9640-4998-84cc-71960c627272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Environment Configuration\n",
    "# Imports necessary libraries for Deep Reinforcement Learning (DRL), numerical computation, and visualization.\n",
    "\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35366c49-911f-42e0-aaaf-750593c09876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Device Configuration\n",
    "# Check for CUDA availability to enable GPU acceleration for tensor operations.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd57ef6-81c3-47cb-b863-3431ad1339a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Module Import from Definition Notebook\n",
    "# Imports functions and classes directly from the definition notebook.\n",
    "import import_ipynb\n",
    "from parity_definition import qc  # Imports the Quantum Environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788a588f-64b7-4f21-980c-4567263279fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Experience Replay Memory\n",
    "# Implements a cyclic buffer to store transitions (experiences) for off-policy training.\n",
    "# This breaks temporal correlations in the data sequence, stabilizing the learning process.\n",
    "\n",
    "# Define a named tuple to represent a single transition in the MDP.\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \"\"\"\n",
    "    A cyclic buffer that stores experiences for Experience Replay.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"\n",
    "        Initializes the replay memory with a fixed capacity.\n",
    "        Args:\n",
    "            capacity (int): Maximum number of transitions to store.\n",
    "        \"\"\"\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"\n",
    "        Saves a transition to memory.\n",
    "        \"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Randomly samples a batch of transitions from memory.\n",
    "        Required for Stochastic Gradient Descent (SGD) updates.\n",
    "        \"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the current number of elements in memory.\n",
    "        \"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f37681-ec8d-4afd-8791-49049f2ae1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Deep Q-Network (DQN) Model Definition\n",
    "# Defines the neural network architecture used to approximate the Q-value function.\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    A Multi-Layer Perceptron (MLP) serving as the function approximator for Q-learning.\n",
    "    Maps state observations to Q-values for each possible action.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        \"\"\"\n",
    "        Initializes the network layers.\n",
    "        Args:\n",
    "            n_observations: Dimension of the input state space.\n",
    "            n_actions: Dimension of the action space.\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.flatten = nn.Flatten() # Flattens multidimensional input tensors\n",
    "        self.layer1 = nn.Linear(n_observations, 256) # Hidden Layer 1: Input -> 256 units\n",
    "        self.layer2 = nn.Linear(256, 256)            # Hidden Layer 2: 256 -> 256 units\n",
    "        self.layer3 = nn.Linear(256, n_actions)      # Output Layer: 256 -> Action Q-values\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \"\"\"\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.layer1(x)) # Activation function: ReLU\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)      # Returns raw Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3ceb8a-3b2b-4d42-8380-570fb372a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Hyperparameters Configuration\n",
    "# Defines key parameters controlling the Reinforcement Learning process.\n",
    "\n",
    "BATCH_SIZE = 128     # Number of transitions sampled from replay memory per update step\n",
    "GAMMA = 0.8          # Discount factor for future rewards\n",
    "EPS_START = 0.9      # Initial Epsilon value for epsilon-greedy policy (High exploration)\n",
    "EPS_END = 0.05       # Minimum Epsilon value (Low exploration)\n",
    "EPS_DECAY = 200      # Rate of Epsilon decay\n",
    "TAU = 0.005          # Soft update rate for target network parameters\n",
    "LR = 0.001           # Learning Rate for the Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf6d79a-b3f8-49e7-b1f7-84db3565964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Environment Initialization\n",
    "# Sets up the quantum circuit environment and defines state/action dimensions.\n",
    "\n",
    "# Initialize the RL environment (Parity Problem)\n",
    "env = qc()\n",
    "\n",
    "n_actions = env.act_space \n",
    "env.reset()\n",
    "\n",
    "# Define Observation Space Size\n",
    "# Flattens the observation matrix (Circuit Depth x Gate Features) into a 1D vector.\n",
    "n_observations = len(env.obs * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e87d09-1648-49ee-83dc-958380707906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Network Initialization and Optimizer Setup\n",
    "# Initializes the Policy Network and Target Network for the DQN algorithm.\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict()) # Synchronize initial weights\n",
    "\n",
    "# Optimizer: AdamW (Adam with Weight Decay Fix)\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "# Initialize Experience Replay Buffer\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0 # Global step counter for Epsilon decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a69735-8e55-4e7d-b680-bca1e73a99e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Action Selection Strategy (Epsilon-Greedy)\n",
    "# Implements the policy for selecting actions based on the current state.\n",
    "\n",
    "def select_action(state):\n",
    "    \"\"\"\n",
    "    Selects an action according to the Epsilon-Greedy policy.\n",
    "    \"\"\"\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    \n",
    "    # Calculate Epsilon Threshold (Exponential Decay)\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    \n",
    "    # Exploitation: Select action with highest Q-value\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    # Exploration: Select random action\n",
    "    else:\n",
    "        return torch.tensor([[env.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200154ae-b0fc-4e82-a785-0146955ffecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Model Optimization Function\n",
    "# Performs a single step of Stochastic Gradient Descent (SGD) to update the Policy Network.\n",
    "\n",
    "def optimize_model():\n",
    "    \"\"\"\n",
    "    Optimizes the policy network by minimizing the loss between predicted Q-values and target Q-values.\n",
    "    \"\"\"\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    # Sample a random batch of transitions\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Current Q-Values\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    # Compute Next State Values using Target Network\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values = target_net(torch.cat(batch.next_state)).max(1)[0]\n",
    "\n",
    "    # Compute Expected Q-Values (Bellman Target)\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Loss (Huber Loss)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimization Step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100) # Gradient Clipping\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1774e06e-bdd8-4086-917a-a9361289147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Logging Initialization\n",
    "# Initializes lists to track performance metrics across all training episodes.\n",
    "\n",
    "reward_ep_list = []      # History of max rewards per episode\n",
    "reward_sum_ep_list = []  # History of cumulative rewards per episode\n",
    "obs_ep_list = []         # History of best circuit structures\n",
    "outs_ep_list = []        # History of optimization logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5341c9-3fd4-4ff0-b71e-681de5be78f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Training Configuration\n",
    "num_episodes = 5000 # Total number of training episodes for the Parity problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baefd799-d716-4d1a-b2f8-588ab128182c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 13: Main Training Loop\n",
    "# Executes the reinforcement learning process.\n",
    "\n",
    "# %%time\n",
    "for i_episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    state = env.obs\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    reward_list = []; obs_list = []; outs_list = []\n",
    "    \n",
    "    for t in count():\n",
    "        # 1. Action Selection\n",
    "        action = select_action(state)\n",
    "        truncated = not env.step(action.item())\n",
    "\n",
    "        if truncated:\n",
    "            print('truncated error')\n",
    "            break\n",
    "        \n",
    "        # 2. Environment Step\n",
    "        observation = env.obs\n",
    "        reward = env.reward\n",
    "        terminated = env.term\n",
    "        done = env.done\n",
    "\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        # 3. Store Transition\n",
    "        memory.push(state, action, next_state, reward, done)\n",
    "        state = next_state\n",
    "\n",
    "        # 4. Optimize Model\n",
    "        optimize_model()\n",
    "\n",
    "        # 5. Soft Update Target Network\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        \n",
    "        # 6. Logging\n",
    "        reward_list.append(reward)\n",
    "        obs_list.append(env.draw)\n",
    "        outs_list.append(env.outs)\n",
    "        \n",
    "        # 7. Episode Termination\n",
    "        if terminated:\n",
    "            max_value = max(reward_list)\n",
    "            max_index = reward_list.index(max_value)\n",
    "            \n",
    "            reward_ep_list.append(max_value)\n",
    "            reward_sum_ep_list.append(sum(reward_list))\n",
    "            obs_ep_list.append(obs_list[max_index])\n",
    "            outs_ep_list.append(outs_list[max_index])\n",
    "            \n",
    "            print(\"Episode complete : \", i_episode+1,\"(\", t+1, \")\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e13d19-f3ba-4ecb-9fb5-97c98d4bfd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Data Persistence Setup\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b6651a-0634-45d0-8a14-0dc6d3a8efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Saving Experiment Results\n",
    "# Serializes and saves the training results to disk.\n",
    "\n",
    "with open('reward_ep_list.pkl', 'wb') as file: pickle.dump(reward_ep_list, file)\n",
    "with open('reward_sum_ep_list.pkl', 'wb') as file: pickle.dump(reward_sum_ep_list, file)\n",
    "with open('obs_ep_list.pkl', 'wb') as file: pickle.dump(obs_ep_list, file)\n",
    "with open('outs_ep_list.pkl', 'wb') as file: pickle.dump(outs_ep_list, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
